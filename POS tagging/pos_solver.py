#!/usr/bin/env python3
###################################
# CS B551 Fall 2018, Assignment #3
#
# Your names and user ids:
#   Dhruuv Agarwal : dagarwa
#   Abhilash Kuhikar : akuhikar
#   Darshan Shinde : dshinde
 
# (Based on skeleton code by D. Crandall)
#
#
####
"""
Result

==> So far scored 2000 sentences with 29442 words.
                   Words correct:     Sentences correct:
   0. Ground truth:      100.00%              100.00%
         1. Simple:       93.92%               47.40%
            2. HMM:       95.06%               54.45%
        3. Complex:       95.32%               56.05%
Time less than 2 mins


Function: train()
here we find the emission, transition, initial, pos tag, and 

P(W|S) --Emission is dictionary w: {s:occ} where occ is occurence and is normalized later for each s or Pos tag
P(S|S-1) -- transition is dictionary prevs: {s:occ} and then normalized over s or Pos tag
P(S1) -- initial is dictionary for pos tag: occ at first position in sentence
P(S) -- pos tag is distribution of each pos tag in whole document, so its each each s or pos tag occurences over all Pos tags 

For complex model we also build another variable called complex trans

Its also a dictionary for storing elements like -P(S3|S1S2)
so type is : s-2s-1:s where s-2 s-1 combinations map to current s
This is also normalized over each combination

We have handled ghost or new words by taking there probability to be:
self.new_word=0.001/sum(self.pos.values())

Here 1/sum(self.pos.values()) represents 1 occurence in total occurences in document and we multiply by further 1e-3 to reduce it further
 

Models:-
1. simplified:
    Naive bayes aproach where each word pos tag depends only on current element and no dependency on past elements
    so we use P(W|S)*P(S) and take the max value to simulate 

2. MCMC:
    Here we use complex model than viterbi and use past two tasg to determins the tag for ccurrent word.
    This we do using sampling not deterministically.
    We use the cooncept of gibbs samplin where we sample one variable by keeping others constant.
    
    Here in our case, in each iteration we take one word by  other. 
    for each word we toggle the pos tag to the 12 values to get possible joint probabilities .
    with this we add to get marginal probability of rest and use it to get conditional probability.
    
    Finally we sample based on this conditional of the remaining elements/tags to get a new sample. 
    
    In mcmc we have a burn period which allows time for samples to approximate true distribution.
    Thus after that we collect samples and find common tag for each word in sentence. This lis tof common tags is our final output.
    
    
    Our mcmc approach is faster and better than other because :
        1. initial smaple isnt random, and is derived from emission probability. This helps in reaching staionary distribution faster.
        2. we calculate the joint probability of the markov field around the current changed element not the whole, this speedends the process.
        3. for ghost words we dont have emission proabbilty so we do a bit of word processing to get some cases of words like words with ed at end are more likely to be verb .
        

To calculate posterior probaility we find the joint probability give the output tag sequence and word sequence in sentence.
This we calculate fpr each output generated by each algorithm and according to each bayes net model.
P(S1S2S3...|W1W2W3...) = P(JOINT)/P(W1W2W3..)

"""
####

import random
import math
import numpy as np
import re
from scipy.stats import mode

# We've set up a suggested code structure, but feel free to change it. Just
# make sure your code still works with the label.py and pos_scorer.py code
# that we've supplied.
#
class Solver:
    # Calculate the log of the posterior probability of a given sentence
    #  with a given part-of-speech labeling. Right now just returns -999 -- fix this!
    pos_init = {}
    transition = {}
    emission = {}
    pos = {}
    mapPOS = {}
    inv_mapPOS = {}
    transitionMatrix = np.zeros((12,12))
    complex_trans = {}
    new_word = 1e-6
    def posterior(self, model, sentence, label):
        if model == "Simple":
            return self.post_simplified(label,sentence)
        elif model == "Complex":
            return self.post_complex(label,sentence)
        elif model == "HMM":
            return self.post_viterbi(label,sentence)
        else:
            print("Unknown algo!")

    # Do the training!
    #
    def train(self, data):
         
        
        for x in data:
            a,b = x
            prevs = ""
            prevs2 = ""
            prevw = ""
            for w,s in zip(a,b):

                self.pos[s] = self.pos.get(s, 0) + 1

                if prevs == "":     self.pos_init[s] = self.pos_init.get(s, 0) + 1
                else:
                    if prevs not in self.transition:  self.transition[prevs] = {tmp: 0 for tmp in ['adj','adv','adp','conj','det','noun','num','pron','prt','verb','x','.']}
                    self.transition[prevs][s] = self.transition[prevs].get(s, 0) + 1
                
                if w not in self.emission:
                    self.emission[w] = {tmp: 0 for tmp in ['adj','adv','adp','conj','det','noun','num','pron','prt','verb','x','.']}
                self.emission[w][s] += 1
                
                if(prevs2!=""):
                    #self.complex_trans[prevs2+prevs] = {'adj':0,'adv':0,'adp':0,'conj':0,'det':0,'noun':0,'num':0,'pron':0,'prt':0,'verb':0,'x':0,'.':0}
                    if prevs2+prevs not in self.complex_trans:  self.complex_trans[prevs2+prevs]={tmp: 0 for tmp in ['adj','adv','adp','conj','det','noun','num','pron','prt','verb','x','.']}
                    self.complex_trans[prevs2+prevs][s] = self.complex_trans[prevs2+prevs].get(s, 0) + 1
                
                prevs2 = prevs                                  
                prevs = s
                prevw = w
                    
        self.new_word=0.001/sum(self.pos.values())
                
        for w,val in self.emission.items():   #as word given pos tag s, so we normalize based on s
            for s in val.keys():
                val[s]= val[s]/self.pos[s] if val[s] !=0 else self.new_word
        
        for s1,val in self.transition.items():  ## modelled as new s or Si+1|Si so normalized on prev s or Si
            total =sum(val.values())                               
            for s2 in val:
                val[s2] = val[s2]/total if val[s2] !=0 else self.new_word

        for s2s1, val in self.complex_trans.items():
            total = sum(val.values())
            for s in val:
                val[s]= val[s]/total  if val[s] !=0 else self.new_word
        
        
        for i1 in self.pos.keys():
            for i2 in self.pos.keys():                                               #self.new_word                               
                if i1+i2 not in self.complex_trans: self.complex_trans[i1+i2] = {tmp:self.new_word for tmp in self.pos.keys()}
        total = sum(self.pos_init.values())
        for s in self.pos_init.keys():  ## generate the initial probabilities
            self.pos_init[s] = self.pos_init[s]/total

        i = 0
        for key in self.transition.keys():
            self.mapPOS[key] = i
            i += 1
        
        self.inv_mapPOS = {value : key for key,value in self.mapPOS.items()}

        for key1 in self.transition:
            for key2 in self.transition[key1]:
                self.transitionMatrix[self.mapPOS[key1]][self.mapPOS[key2]] = self.transition[key1][key2]



    # Functions for each algorithm. Right now this just returns nouns -- fix this!
    #
    def simplified(self, sentence):
        pred = []
        for w in sentence:
            try:
                tmpdict={s:times*self.pos[s] for s,times in self.emission[w].items()}
                pred.append(max(tmpdict,key=tmpdict.get))
            except:
                pred.append(max(self.pos,key=self.pos.get))        
            
        return pred

    def random_pick(self,d):
        r = random.uniform(0.00, 1.00)
        cdf = 0
        for s, prob in d.items():
            cdf += prob
            if cdf >= r:
                return s
        return s 
            
    def estimate_emission(self,word,s):
        
        if re.compile("\d").search(word) and s=="num":
            #print("\n\nnum detect\n")
            return 100
        elif word.endswith("ed") and s=="verb":
            #print("\n\nverb detect\n")
            return 100
        
        return 1
    
    def whatif_joint(self,l,sentence,s,s_pred):
        joint =1
        if sentence[l] not in self.emission and sentence[l].endswith("'s"): sentence[l] = sentence[l].replace("'s","")
        if l>=2:
            try:
                new_factor_l = (self.complex_trans[s_pred[l-2]+s_pred[l-1]][s] *self.emission[sentence[l]][s])
            except KeyError:
                new_factor_l = self.new_word*(self.complex_trans[s_pred[l-2]+s_pred[l-1]][s]) if s in self.complex_trans[s_pred[l-2]+s_pred[l-1]] else self.new_word*self.new_word
                new_factor_l*= self.estimate_emission(sentence[l],s)                                    
            joint *= new_factor_l
            if l+1 <len(sentence): joint *= self.complex_trans[ s_pred[l-1]+s ] [s_pred[l+1]] if s_pred[l+1] in self.complex_trans[ s_pred[l-1]+s ] else self.new_word 
            if l+2 <len(sentence): joint *= self.complex_trans[ s+s_pred[l+1] ] [s_pred[l+2]] if s_pred[l+2] in self.complex_trans[ s+s_pred[l+1] ] else self.new_word           
        elif l==1:
            try:
                new_factor_l = (self.transition[s_pred[l-1]][s] *self.emission[sentence[l]][s])
            except:
                new_factor_l = (self.transition[s_pred[l-1]][s] *self.new_word) if s in self.transition[s_pred[l-1]] else self.new_word*self.new_word  
                new_factor_l *= self.estimate_emission(sentence[l],s)               
            joint *= new_factor_l
            if l+1 <len(sentence): joint *= self.complex_trans[ s_pred[l-1]+s ] [s_pred[l+1]] if s_pred[l+1] in self.complex_trans[ s_pred[l-1]+s ] else self.new_word
            if l+2 <len(sentence): joint *= self.complex_trans[ s+s_pred[l+1] ] [s_pred[l+2]] if s_pred[l+2] in self.complex_trans[ s+s_pred[l+1] ] else self.new_word
        else:
            try:
                val = self.pos_init[s]*self.emission[sentence[l]][s]
            except KeyError:
                val= self.pos_init[s]*self.new_word
                val *= self.estimate_emission(sentence[l],s)                  
            try:                      
                if l+1 <len(sentence): joint *= self.transition[s] [s_pred[l+1]]  
            except:
                joint*=self.new_word
            if l+2 <len(sentence): joint *= self.complex_trans[ s+s_pred[l+1] ] [s_pred[l+2]] if s_pred[l+2] in self.complex_trans[ s+s_pred[l+1] ] else self.new_word   
            joint *=val if val!=0 else self.new_word
            
        return joint
    
    def post_simplified(self,sseq,sentence):
        n= len(sentence)
        jsum = 0
        l=0
        while(l<n):
            try:
                zer_val = self.pos_init[sseq[l]]*self.emission[sentence[l]][sseq[l]]
            except KeyError:
                zer_val= self.pos_init[sseq[l]]*self.new_word
                zer_val *= self.estimate_emission(sentence[l],sseq[l])
            jsum += np.log(zer_val)
            l+=1
        return jsum    
    
    def post_viterbi(self,sseq,sentence):
        n= len(sentence)
        jsum = 0
        if n>0:
            try:
                zer_val = self.pos_init[sseq[0]]*self.emission[sentence[0]][sseq[0]]
            except KeyError:
                zer_val= self.pos_init[sseq[0]]*self.new_word
                zer_val *= self.estimate_emission(sentence[0],sseq[0])
            jsum += np.log(zer_val)
            l=1
            while(l<n):
                try:
                    first_l = (self.transition[sseq[l-1]][sseq[l]] *self.emission[sentence[l]][sseq[l]])
                except:
                    first_l = (self.transition[sseq[l-1]][sseq[l]] *self.new_word) if sseq[l] in self.transition[sseq[l-1]] else self.new_word*self.new_word  
                    first_l *= self.estimate_emission(sentence[l],sseq[l])               
                jsum += np.log(first_l)
                l+=1
        return jsum       
    
    def post_complex(self,sseq,sentence):
        n= len(sentence)
        jsum = 0
        if n>0:
            try:
                zer_val = self.pos_init[sseq[0]]*self.emission[sentence[0]][sseq[0]]
            except KeyError:
                zer_val= self.pos_init[sseq[0]]*self.new_word
                zer_val *= self.estimate_emission(sentence[0],sseq[0])
            jsum += np.log(zer_val)
            if n>1:
                try:
                    first_l = (self.transition[sseq[0]][sseq[1]] *self.emission[sentence[1]][sseq[1]])
                except:
                    first_l = (self.transition[sseq[0]][sseq[1]] *self.new_word) if sseq[1] in self.transition[sseq[0]] else self.new_word*self.new_word  
                    first_l *= self.estimate_emission(sentence[1],sseq[1])               
                jsum += np.log(first_l)
                l=2
                while(l<n):
                    try:
                        new_factor_l = (self.complex_trans[sseq[l-2]+sseq[l-1]][sseq[l]] *self.emission[sentence[l]][sseq[l]])
                    except KeyError:
                        new_factor_l = self.new_word*(self.complex_trans[sseq[l-2]+sseq[l-1]][sseq[l]]) if sseq[l] in self.complex_trans[sseq[l-2]+sseq[l-1]] else self.new_word*self.new_word
                        new_factor_l*= self.estimate_emission(sentence[l],sseq[l])                                    
                    jsum += np.log(new_factor_l)    
                    l+=1
        return jsum          
            
    
    
    
    
    def complex_mcmc(self, sentence):
        pred = []
        sentence=list(sentence)
        if len(sentence)==0: return []
        init = [max(self.emission[w],key=self.emission[w].get) if w in self.emission else "noun" for w in sentence] # initialization
        samples = []
        
        #joint = self.calc_joint(sentence,init)
        burn_iter = 20        
        for i in range(0,60):
            loc = random.randint(0,len(sentence)-1)
            for loc in range(0,len(sentence)):
                joint_dict = {s:self.whatif_joint(loc,sentence,s,init) for s in self.pos.keys()}
                #print(joint_dict)
                condit = {s:val/sum(joint_dict.values()) if sum(joint_dict.values()) !=0 else 0 for s, val in joint_dict.items()}
                picked_s = self.random_pick(condit)
                init = init[:loc]+[picked_s]+init[loc+1:]
                if(burn_iter<i):
                    samples.append(init)
        pred = mode(np.array(samples),axis=0)
        return pred[0][0]


    def hmm_viterbi(self, sentence):
        #for w,s in sentence.
        #construct a transition table storing the values
        V = np.zeros((12,len(sentence)))
        i = 0
        backtrack = np.zeros(V.shape)
        for w in sentence:
            
            #initial probabilities
            if i == 0:
                for pos in self.mapPOS.keys():
                    emission = 0
                    if w not in self.emission.keys():
                        emission = self.new_word#1e-5
                    else:
                        emission = self.emission[w][pos]
                    V[self.mapPOS[pos],i] = np.log(self.pos_init[pos] * emission)
            else:
                #transition to next state
                for pos in self.mapPOS.keys():
                    emission = 0
                    if w not in self.emission.keys():
                        emission = -100
                    else:
                        emission = np.log(self.emission[w][pos])
                    vector = np.log(self.transitionMatrix.T[self.mapPOS[pos],:].reshape(1,-1)) + \
                                V[:,i-1].reshape(1,-1)
                    best = np.argmax(vector)
                    backtrack[self.mapPOS[pos],i] = best
                    V[self.mapPOS[pos],i] = np.max(vector) + emission
            i += 1
        #backtracking
        prev_best = np.argmax(V[:,-1])
        best_sequence = [self.inv_mapPOS[prev_best]]
        for i in range(len(sentence)-1):
            index = len(sentence) - i - 1
            prev_best = int(backtrack[prev_best,index])
            best_sequence.append(self.inv_mapPOS[prev_best])
        best_sequence.reverse()
        return best_sequence


    # This solve() method is called by label.py, so you should keep the interface the
    #  same, but you can change the code itself. 
    # It should return a list of part-of-speech labelings of the sentence, one
    #  part of speech per word.
    #
    def solve(self, model, sentence):
        if model == "Simple":
            return self.simplified(sentence)
        elif model == "Complex":
            return self.complex_mcmc(sentence)
        elif model == "HMM":
            return self.hmm_viterbi(sentence)
        else:
            print("Unknown algo!")

